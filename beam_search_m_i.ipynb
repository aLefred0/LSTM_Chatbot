{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ar6bKf7iUOc0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import LSTM,Dense, Concatenate,Bidirectional,Input,LSTMCell\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy,CosineSimilarity\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow_addons.seq2seq import BeamSearchDecoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "gpNrtZqSUOdW",
    "outputId": "02a11949-b066-4ac1-e3c4-54c5b55c7f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'abandon' 'ability' ... '<start>' '<end>' '<pad>']\n",
      "5035\n"
     ]
    }
   ],
   "source": [
    "#importing vocabulary\n",
    "ds_path='/home/al/Desktop/nnfl/word2vecmod.txt'\n",
    "data = np.loadtxt(ds_path,dtype=str,usecols=(0))\n",
    "print(data)\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceoDkt65UOd0"
   },
   "outputs": [],
   "source": [
    "#loading word vectors for getting unkown words\n",
    "tmp_file = get_tmpfile(datapath(\"/home/al/Desktop/nnfl/new\"))\n",
    "word_model = KeyedVectors.load(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtQpShsKUOeD"
   },
   "outputs": [],
   "source": [
    "#list of words in word vectors\n",
    "data2= np.loadtxt('/home/al/Desktop/nnflproj/glove.6B.300d.txt',dtype=str,usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF6FJ7DtUOeT"
   },
   "outputs": [],
   "source": [
    "#making word-to-index and index-to-word dictionaries\n",
    "word_to_ix=dict(zip(data,range(0,data.shape[0])))\n",
    "ix_to_word=dict(zip(range(0,data.shape[0]),data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEmS5cI5UOey"
   },
   "outputs": [],
   "source": [
    "#constants\n",
    "voc=data.shape[0]#for <start>,<end>,<pad>\n",
    "ip_len=15\n",
    "op_len=15\n",
    "n_s=64+op_len#no of decoder lstm\n",
    "n_a=64#no \n",
    "batch_size=20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-sBS7U3UOfN"
   },
   "outputs": [],
   "source": [
    "#one-hot function\n",
    "def oh(Xq,ip_ln):\n",
    "    Xoh=np.zeros([Xq.shape[0],ip_ln,voc])\n",
    "    for i in range(Xq.shape[0]):\n",
    "        for j in range(ip_ln):\n",
    "            Xoh[i,j,int(Xq[i,j])]=1\n",
    "    return Xoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0vfFHwiUOhh"
   },
   "outputs": [],
   "source": [
    "def ev(s1):\n",
    "        s1='<start> '+s1+' <end>'\n",
    "        Xt=np.zeros(shape=[ip_len])\n",
    "        #ds.shape[0]-(ds.shape[0]%1000)+1)\n",
    "        w1=np.array(s1.lower().split())\n",
    "        for i in range(0,ip_len):    \n",
    "            w=w1[i]\n",
    "            if i==ip_len-1:\n",
    "                #w1_sum/=50\n",
    "                Xt[i]=voc-2#for<end>\n",
    "                \n",
    "                break\n",
    "            if w=='<start>':\n",
    "                Xt[i]=voc-3\n",
    "                continue\n",
    "            if w=='<end>':\n",
    "                Xt[i]=voc-2#<end>\n",
    "                i+=1\n",
    "                while i<ip_len:\n",
    "                    Xt[i]=voc-1#padding\n",
    "                    i+=1\n",
    "                break\n",
    "            if w in data:                \n",
    "                Xt[i]=word_to_ix[w]\n",
    "            else:\n",
    "                try:\n",
    "                    temp=word_model.most_similar(positive=[w],negative=[],restrict_vocab=4095)#for<unk>\n",
    "                    \n",
    "                    Xt[i]=word_to_ix[temp[0][0]]\n",
    "                except KeyError:\n",
    "                    Xt[i]=word_to_ix['it']\n",
    "                    \n",
    "            return Xt.reshape([1,ip_len])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "O5Q1Psi0fLc0",
    "outputId": "1a4bda2a-faab-45fc-ad5c-837ef47ef272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "dull_oh=np.ndarray(shape=[4,15,5035])\n",
    "\n",
    "dull_sen=['i do not know','ok','i have no idea','maybe']\n",
    "i=0\n",
    "for el in dull_sen:\n",
    "  a=ev(el)\n",
    "  a=oh(a,ip_len)\n",
    "  dull_oh[i]=a\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "W-r6DS3lQZvj",
    "outputId": "d18e983c-ff1c-44c6-eedc-91fc173f399e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 15, 5035)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 15, 100)      2054400     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  (None, 15, 100)      80400       lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  (None, 15, 100)      80400       lstm_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 15, 5035)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  [(None, 15, 1000), ( 4404000     lstm_31[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,619,200\n",
      "Trainable params: 6,619,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "enc_ip=Input(shape=(ip_len,voc))\n",
    "enc_lstm1=LSTM(100,return_sequences=True)\n",
    "enc_lstm2=LSTM(100,return_sequences=True)\n",
    "enc_lstm3=LSTM(100,return_sequences=True)\n",
    "enc_lstm4=LSTM(1000,return_state=True,return_sequences=True)\n",
    "en=enc_lstm1(enc_ip)\n",
    "en=enc_lstm2(en)\n",
    "en=enc_lstm3(en)\n",
    "enc_op,h,c=enc_lstm4(en)\n",
    "concat=Concatenate(axis=-1)\n",
    "enc_states=[h,c]\n",
    "dec_ip=Input(shape=(op_len,voc))\n",
    "dense=Dense(voc,activation='softmax')\n",
    "dec_lstm=LSTM(1000,return_sequences=True,return_state=True)\n",
    "dec_op,_,_=dec_lstm(dec_ip,initial_state=enc_states)\n",
    "\n",
    "op=dense(dec_op)\n",
    "model=Model([enc_ip,dec_ip],enc_op)\n",
    "\n",
    "model.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 15, 5035)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 15, 100)           2054400   \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 15, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 15, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               [(None, 15, 1000), (None, 4404000   \n",
      "=================================================================\n",
      "Total params: 6,619,200\n",
      "Trainable params: 6,619,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "enc_model=Model(enc_ip,enc_op)\n",
    "enc_model.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(enc_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "bS8x4MIXUOfw",
    "outputId": "149a266e-0e0f-42d2-a88a-d546db3f02cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 8.6683085e-06  1.0323745e-05  1.3909574e-05 ...  2.8876404e-06\n",
      "    7.9924148e-06  3.5765036e-06]\n",
      "  [ 2.1260574e-05  3.4408182e-05  3.2315482e-05 ...  1.0514347e-05\n",
      "    1.8351093e-05  5.0931635e-06]\n",
      "  [ 2.3842460e-05  6.2993597e-05  4.8856687e-05 ...  2.3534401e-05\n",
      "    3.1118110e-05  1.6299377e-06]\n",
      "  ...\n",
      "  [-4.3421233e-04 -8.9967623e-04 -3.3774049e-04 ...  3.9662290e-04\n",
      "    6.2591641e-04 -1.4626371e-05]\n",
      "  [-4.1793531e-04 -1.0709496e-03 -4.1476317e-04 ...  4.1842458e-04\n",
      "    7.2325225e-04 -1.0395716e-05]\n",
      "  [-3.8401029e-04 -1.2351057e-03 -4.9052044e-04 ...  4.3263086e-04\n",
      "    8.2021009e-04 -8.5474794e-06]]]\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "c=enc_model.predict(oh(ev('hi'),ip_len))\n",
    "a=tf.transpose(c,perm=[1,0,2])\n",
    "seq_len=[ip_len]\n",
    "paths=3\n",
    "decop,log_prob=tf.nn.ctc_beam_search_decoder(a,seq_len,beam_width=5,top_paths=paths)\n",
    "d=[]\n",
    "for i in range(paths):\n",
    "      d+=[decop[i].values]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([], shape=(0,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=oh(ev('hi'),ip_len)\n",
    "a2=tf.transpose(c,perm=[1,0,2])\n",
    "decop,log_pro=tf.nn.ctc_beam_search_decoder(a2,seq_len,beam_width=5,top_paths=paths)\n",
    "d2=[]\n",
    "for i in range(paths):\n",
    "    d2+=[decop[i].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(3,), dtype=int64, numpy=array([5032, 1258,    0])>, <tf.Tensor: shape=(4,), dtype=int64, numpy=array([5032, 1258,    0,    3])>, <tf.Tensor: shape=(4,), dtype=int64, numpy=array([5032, 1258,    0,    2])>]\n"
     ]
    }
   ],
   "source": [
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_pro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-489e4a9fe815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_pro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_pro' is not defined"
     ]
    }
   ],
   "source": [
    "print(log_pro[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUkU_8nCUOgE"
   },
   "outputs": [],
   "source": [
    "#creating checkpoint\n",
    "filepath=\"/home/al/Desktop/nnfl final/weightsfinv100.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')\n",
    "#model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skPjRQuIUOgn",
    "outputId": "aa6cd30d-47a7-4fc9-87be-2a5c726d858b",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "4109\n",
      "389636\n"
     ]
    }
   ],
   "source": [
    "d_path='/home/al/Desktop/nnfl final/nds-a'\n",
    "idx=0\n",
    "idx2=0\n",
    "p=3\n",
    "for e  in 'a':\n",
    "#bcdefghi':\n",
    "\n",
    "    for f  in 'abc':\n",
    "        #defghijklmnopqrstuvwxyz':\n",
    "       \n",
    "        ds = np.loadtxt(d_path+e+f+'.csv',delimiter='\\n',dtype='str')\n",
    "        num_sen=ds.shape[0]\n",
    "        idx=0\n",
    "        Xtr=np.ndarray(shape=[num_sen,ip_len])\n",
    "        for j in range(0,num_sen):\n",
    "                #ds.shape[0]-(ds.shape[0]%1000)+1)\n",
    "                w1=np.array(ds[j].lower().split())\n",
    "                for i in range(0,ip_len):    \n",
    "                    w=w1[i]\n",
    "                    if i==ip_len-1:\n",
    "                        #w1_sum/=50\n",
    "                        Xtr[j,i]=voc-2#for<end>\n",
    "                        \n",
    "                        break\n",
    "                    if w=='<start>':\n",
    "                        Xtr[j,i]=voc-3\n",
    "                        continue\n",
    "                    if w=='<end>':\n",
    "                        Xtr[j,i]=voc-2#<end>\n",
    "                        i+=1\n",
    "                        while i<ip_len:\n",
    "                            Xtr[j,i]=voc-1#padding\n",
    "                            i+=1\n",
    "                        break\n",
    "                    if w in data:                \n",
    "                        Xtr[j,i]=word_to_ix[w]\n",
    "                    else:\n",
    "                        try:\n",
    "                            temp=word_model.most_similar(positive=[w],negative=[],restrict_vocab=4095)#for<unk>\n",
    "                            idx2+=1\n",
    "                            Xtr[j,i]=word_to_ix[temp[0][0]]\n",
    "                        except KeyError:\n",
    "                            Xtr[j,i]=word_to_ix['it']\n",
    "                            idx+=1\n",
    "\n",
    "\n",
    "                        \n",
    "                           \n",
    "                if j%1000==0:\n",
    "                    print(j)\n",
    "    print(idx)\n",
    "    print(idx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qx4kFiVLUOg5",
    "outputId": "a465b588-59ba-4388-9722-948040329fd9",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0884 - accuracy: 0.5383 - val_loss: 2.6375 - val_accuracy: 0.5733\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2532 - accuracy: 0.5258 - val_loss: 2.7716 - val_accuracy: 0.5800\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8199 - accuracy: 0.5550 - val_loss: 2.3967 - val_accuracy: 0.6367\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.7824 - accuracy: 0.5808 - val_loss: 2.5187 - val_accuracy: 0.6333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4502 - accuracy: 0.5042 - val_loss: 2.3640 - val_accuracy: 0.6267\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.6052 - accuracy: 0.6042 - val_loss: 2.7636 - val_accuracy: 0.5333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.2371 - accuracy: 0.6383 - val_loss: 2.9565 - val_accuracy: 0.5200\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8750 - accuracy: 0.5458 - val_loss: 2.6192 - val_accuracy: 0.6200\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.7271 - accuracy: 0.5767 - val_loss: 2.9229 - val_accuracy: 0.5500\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 5.9466 - accuracy: 0.2200 - val_loss: 6.3966 - val_accuracy: 0.1433\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 6.1123 - accuracy: 0.1567 - val_loss: 5.4692 - val_accuracy: 0.1733\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 5.3861 - accuracy: 0.2033 - val_loss: 5.9608 - val_accuracy: 0.1633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 5.7850 - accuracy: 0.1425 - val_loss: 5.2474 - val_accuracy: 0.2167\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.3317 - accuracy: 0.3683 - val_loss: 3.7506 - val_accuracy: 0.4633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.5698 - accuracy: 0.4642 - val_loss: 2.7661 - val_accuracy: 0.5767\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.9475 - accuracy: 0.4183 - val_loss: 3.6001 - val_accuracy: 0.4267\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.7685 - accuracy: 0.4183 - val_loss: 3.4618 - val_accuracy: 0.4633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.1866 - accuracy: 0.4950 - val_loss: 3.9484 - val_accuracy: 0.3900\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.3045 - accuracy: 0.4742 - val_loss: 3.0959 - val_accuracy: 0.4867\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4970 - accuracy: 0.4567 - val_loss: 4.0008 - val_accuracy: 0.3567\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.7519 - accuracy: 0.4408 - val_loss: 4.0476 - val_accuracy: 0.3767\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2954 - accuracy: 0.4758 - val_loss: 2.6474 - val_accuracy: 0.5700\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.3865 - accuracy: 0.4800 - val_loss: 4.2202 - val_accuracy: 0.3700\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8908 - accuracy: 0.5292 - val_loss: 3.3748 - val_accuracy: 0.4367\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.6700 - accuracy: 0.5600 - val_loss: 1.9037 - val_accuracy: 0.6633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8552 - accuracy: 0.5250 - val_loss: 2.4032 - val_accuracy: 0.5933\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.7200 - accuracy: 0.5442 - val_loss: 3.2642 - val_accuracy: 0.4600\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.4840 - accuracy: 0.5825 - val_loss: 4.0306 - val_accuracy: 0.3700\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.5627 - accuracy: 0.5783 - val_loss: 2.8855 - val_accuracy: 0.5600\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.8629 - accuracy: 0.2925 - val_loss: 6.0180 - val_accuracy: 0.2200\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.6552 - accuracy: 0.4942 - val_loss: 3.7351 - val_accuracy: 0.5100\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.3326 - accuracy: 0.3975 - val_loss: 4.2076 - val_accuracy: 0.4333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0625 - accuracy: 0.5217 - val_loss: 3.6877 - val_accuracy: 0.3833\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.9013 - accuracy: 0.5108 - val_loss: 2.8229 - val_accuracy: 0.5300\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8426 - accuracy: 0.5458 - val_loss: 2.4894 - val_accuracy: 0.5633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4487 - accuracy: 0.4450 - val_loss: 2.9541 - val_accuracy: 0.5100\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.2357 - accuracy: 0.6483 - val_loss: 3.9378 - val_accuracy: 0.3767\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0087 - accuracy: 0.5108 - val_loss: 2.6362 - val_accuracy: 0.6100\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4924 - accuracy: 0.4425 - val_loss: 3.5692 - val_accuracy: 0.3967\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.1761 - accuracy: 0.4758 - val_loss: 3.4173 - val_accuracy: 0.4033\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.5278 - accuracy: 0.4183 - val_loss: 3.6890 - val_accuracy: 0.3633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.3415 - accuracy: 0.4325 - val_loss: 2.6124 - val_accuracy: 0.5533\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.1696 - accuracy: 0.4800 - val_loss: 2.6838 - val_accuracy: 0.5733\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2236 - accuracy: 0.4608 - val_loss: 3.8700 - val_accuracy: 0.3900\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.1023 - accuracy: 0.4667 - val_loss: 2.8778 - val_accuracy: 0.5000\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.9675 - accuracy: 0.3817 - val_loss: 5.4907 - val_accuracy: 0.2433\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.4790 - accuracy: 0.3325 - val_loss: 2.9543 - val_accuracy: 0.5200\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.3554 - accuracy: 0.4708 - val_loss: 3.7774 - val_accuracy: 0.3867\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4608 - accuracy: 0.4617 - val_loss: 4.8338 - val_accuracy: 0.3233\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.2708 - accuracy: 0.3558 - val_loss: 4.3394 - val_accuracy: 0.3900\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.9617 - accuracy: 0.5550 - val_loss: 3.1605 - val_accuracy: 0.5067\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.7270 - accuracy: 0.5558 - val_loss: 2.9362 - val_accuracy: 0.5300\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.6591 - accuracy: 0.5775 - val_loss: 2.2329 - val_accuracy: 0.6333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.4158 - accuracy: 0.6092 - val_loss: 2.3487 - val_accuracy: 0.6167\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.3725 - accuracy: 0.6283 - val_loss: 2.6875 - val_accuracy: 0.5533\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0829 - accuracy: 0.5108 - val_loss: 3.8943 - val_accuracy: 0.4300\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2299 - accuracy: 0.4900 - val_loss: 3.1638 - val_accuracy: 0.5367\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.5917 - accuracy: 0.5917 - val_loss: 2.1325 - val_accuracy: 0.6533\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.9194 - accuracy: 0.5500 - val_loss: 3.7613 - val_accuracy: 0.4033\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2773 - accuracy: 0.4758 - val_loss: 3.4866 - val_accuracy: 0.4600\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.9498 - accuracy: 0.5050 - val_loss: 2.3167 - val_accuracy: 0.6200\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0370 - accuracy: 0.5092 - val_loss: 3.0143 - val_accuracy: 0.5233\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4282 - accuracy: 0.4350 - val_loss: 2.9462 - val_accuracy: 0.5633\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2478 - accuracy: 0.4900 - val_loss: 2.4340 - val_accuracy: 0.6133\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.3064 - accuracy: 0.4817 - val_loss: 3.5247 - val_accuracy: 0.5833\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 5.2878 - accuracy: 0.2450 - val_loss: 4.0770 - val_accuracy: 0.3700\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.5759 - accuracy: 0.4683 - val_loss: 4.1705 - val_accuracy: 0.3800\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.0276 - accuracy: 0.3967 - val_loss: 4.2088 - val_accuracy: 0.3467\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 4.0051 - accuracy: 0.3725 - val_loss: 5.1405 - val_accuracy: 0.2433\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.4919 - accuracy: 0.4508 - val_loss: 3.4226 - val_accuracy: 0.4500\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.7283 - accuracy: 0.4317 - val_loss: 4.2683 - val_accuracy: 0.3967\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.4132 - accuracy: 0.4758 - val_loss: 3.3402 - val_accuracy: 0.4567\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.5447 - accuracy: 0.4800 - val_loss: 3.9451 - val_accuracy: 0.3767\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.3713 - accuracy: 0.4592 - val_loss: 2.7209 - val_accuracy: 0.5600\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.2959 - accuracy: 0.4900 - val_loss: 3.5421 - val_accuracy: 0.4600\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.9887 - accuracy: 0.5417 - val_loss: 3.8315 - val_accuracy: 0.4033\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.9969 - accuracy: 0.5183 - val_loss: 3.1346 - val_accuracy: 0.4867\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0173 - accuracy: 0.5108 - val_loss: 3.7313 - val_accuracy: 0.4067\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2025 - accuracy: 0.4875 - val_loss: 3.0925 - val_accuracy: 0.5100\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.5726 - accuracy: 0.5850 - val_loss: 3.1796 - val_accuracy: 0.4833\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.0286 - accuracy: 0.5125 - val_loss: 3.7574 - val_accuracy: 0.3800\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.2463 - accuracy: 0.4758 - val_loss: 2.9621 - val_accuracy: 0.5567\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8336 - accuracy: 0.5400 - val_loss: 2.8884 - val_accuracy: 0.5200\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8893 - accuracy: 0.5308 - val_loss: 2.9227 - val_accuracy: 0.5367\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.8799 - accuracy: 0.5125 - val_loss: 2.9645 - val_accuracy: 0.5233\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.2788 - accuracy: 0.4525 - val_loss: 2.9670 - val_accuracy: 0.5067\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.2307 - accuracy: 0.4483 - val_loss: 2.8080 - val_accuracy: 0.5233\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.0262 - accuracy: 0.5117 - val_loss: 2.9802 - val_accuracy: 0.5133\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.3291 - accuracy: 0.6133 - val_loss: 3.2929 - val_accuracy: 0.4767\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.2075 - accuracy: 0.4642 - val_loss: 2.6829 - val_accuracy: 0.5667\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.1767 - accuracy: 0.4867 - val_loss: 3.0829 - val_accuracy: 0.4833\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.9067 - accuracy: 0.5117 - val_loss: 2.7339 - val_accuracy: 0.5333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.1892 - accuracy: 0.4625 - val_loss: 3.4743 - val_accuracy: 0.4067\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.0009 - accuracy: 0.5025 - val_loss: 2.8565 - val_accuracy: 0.5333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.8915 - accuracy: 0.5183 - val_loss: 2.7537 - val_accuracy: 0.5367\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 3.1995 - accuracy: 0.4600 - val_loss: 3.2386 - val_accuracy: 0.4433\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.8993 - accuracy: 0.5250 - val_loss: 2.5447 - val_accuracy: 0.5833\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 2.7740 - accuracy: 0.5258 - val_loss: 2.8716 - val_accuracy: 0.5100\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.8856 - accuracy: 0.5100 - val_loss: 2.9353 - val_accuracy: 0.4933\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.1533 - accuracy: 0.4650 - val_loss: 2.9047 - val_accuracy: 0.4800\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.1267 - accuracy: 0.4625 - val_loss: 3.2115 - val_accuracy: 0.4100\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.8111 - accuracy: 0.5217 - val_loss: 2.8939 - val_accuracy: 0.5233\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.5627 - accuracy: 0.5792 - val_loss: 3.0815 - val_accuracy: 0.4833\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 0s - loss: 3.0417 - accuracy: 0.5000 - val_loss: 3.4724 - val_accuracy: 0.4333\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.8666 - accuracy: 0.5367 - val_loss: 3.5477 - val_accuracy: 0.4300\n",
      "Train on 80 samples, validate on 20 samples\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.76333\n",
      "80/80 - 1s - loss: 2.8106 - accuracy: 0.5225 - val_loss: 2.8222 - val_accuracy: 0.5067\n",
      "Train on 80 samples, validate on 20 samples\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-80e2c7867064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mip_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\n\\n\\ndone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "for kl in range(100,num_sen,100):\n",
    "    u=oh(Xtr[kl-100:kl+1],ip_len)\n",
    "    u=tf.convert_to_tensor(u)\n",
    "    model.fit([u[0:-1],u[0:-1]],u[1:,:,:],epochs=1,batch_size=20,use_multiprocessing=True,callbacks=[checkpoint],verbose=2,validation_split=0.2,shuffle=True)\n",
    "print('\\n\\n\\n\\n\\ndone')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "finALv2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit910a7e525ea849fca55756066482ff27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
