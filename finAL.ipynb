{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import LSTM,Dense, Concatenate,Bidirectional,Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['a' 'abandon' 'ability' ... '<start>' '<end>' '<pad>']\n5035\n"
    }
   ],
   "source": [
    "#importing vocabulary\n",
    "ds_path='/home/al/Desktop/nnfl/word2vecmod.txt'\n",
    "data = np.loadtxt(ds_path,dtype=str,usecols=(0))\n",
    "print(data)\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading word vectors for getting unkown words\n",
    "tmp_file = get_tmpfile(datapath(\"/home/al/Desktop/nnfl/new\"))\n",
    "word_model = KeyedVectors.load(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words in word vectors\n",
    "data2= np.loadtxt('/home/al/Desktop/nnflproj/glove.6B.300d.txt',dtype=str,usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making word-to-index and index-to-word dictionaries\n",
    "word_to_ix=dict(zip(data,range(0,data.shape[0])))\n",
    "ix_to_word=dict(zip(range(0,data.shape[0]),data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "voc=data.shape[0]#for <start>,<end>,<pad>\n",
    "ip_len=10\n",
    "op_len=10\n",
    "n_s=64+op_len#no of decoder lstm\n",
    "n_a=64#no \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot function\n",
    "def oh(Xq,ip_ln):\n",
    "    Xoh=np.zeros([Xq.shape[0],ip_ln,voc])\n",
    "    for i in range(Xq.shape[0]):\n",
    "        for j in range(ip_ln):\n",
    "            Xoh[i,j,int(Xq[i,j])]=1\n",
    "    return Xoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 10, 5035)]   0                                            \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   [(None, 10, 128), (N 2611200     input_1[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       multiple             0           bidirectional[0][1]              \n                                                                 bidirectional[0][3]              \n                                                                 bidirectional[0][2]              \n                                                                 bidirectional[0][4]              \n                                                                 lstm_1[0][0]                     \n                                                                 additive_attention[0][0]         \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 10, 5035)]   0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, 10, 128), (N 2643968     input_2[0][0]                    \n                                                                 concatenate[0][0]                \n                                                                 concatenate[1][0]                \n__________________________________________________________________________________________________\nadditive_attention (AdditiveAtt (None, 10, 128)      128         lstm_1[0][0]                     \n                                                                 bidirectional[0][0]              \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 10, 5034)     1293738     concatenate[2][0]                \n==================================================================================================\nTotal params: 6,549,034\nTrainable params: 6,549,034\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "#model\n",
    "enc_ip=Input(shape=(ip_len,voc))\n",
    "enc_lstm=LSTM(64,return_state=True,return_sequences=True)\n",
    "bi=Bidirectional(enc_lstm)\n",
    "enc_op,h1,c1,h2,c2=bi(enc_ip)\n",
    "concat=Concatenate(axis=-1)\n",
    "h=concat([h1,h2])\n",
    "c=concat([c1,c2])\n",
    "enc_states=[h,c]\n",
    "dec_ip=Input(shape=(op_len,voc))\n",
    "dec_lstm=LSTM(128,return_sequences=True,return_state=True)\n",
    "dec_op,_,_=dec_lstm(dec_ip,initial_state=enc_states)\n",
    "att=AdditiveAttention()([dec_op, enc_op])\n",
    "dense=Dense(voc-1,activation='softmax')\n",
    "op=concat([dec_op,att])\n",
    "op=dense(op)\n",
    "model=Model([enc_ip,dec_ip],op)\n",
    "model.compile(optimizer=\"rmsprop\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating checkpoint\n",
    "filepath=\"/home/al/Desktop/nnfl final/weightsfinv6.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='accuracy',verbose=1, save_best_only=True, mode='max')\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n11000\n12000\n13000\n14000\n15000\n16000\n17000\n18000\n19000\n20000\n21000\n22000\n23000\n24000\n25000\n26000\n27000\n28000\n29000\n30000\n31000\n32000\n33000\n34000\n35000\n36000\n37000\n38000\n39000\n40000\n41000\n42000\n43000\n44000\n45000\n46000\n47000\n48000\n49000\n50000\n51000\n52000\n53000\n54000\n55000\n56000\n57000\n58000\n59000\n60000\n61000\n62000\n63000\n64000\n65000\n66000\n67000\n68000\n69000\n70000\n71000\n72000\n73000\n74000\n75000\n76000\n77000\n78000\n79000\n80000\n81000\n82000\n83000\n84000\n85000\n86000\n87000\n88000\n89000\n90000\n91000\n92000\n93000\n94000\n95000\n96000\n97000\n98000\n99000\n4322\n109074\n"
    }
   ],
   "source": [
    "d_path='/home/al/Desktop/nnfl final/nds-a'\n",
    "idx=0\n",
    "idx2=0\n",
    "for e  in 'a':\n",
    "#bcdefghi':\n",
    "\n",
    "    for f  in 'a':\n",
    "        #bcdefghijklmnopqrstuvwxyz':\n",
    "        ds = np.loadtxt(d_path+e+f+'.csv',delimiter='\\n',dtype='str')\n",
    "        num_sen=ds.shape[0]\n",
    "        idx=0\n",
    "        Xtr=np.ndarray(shape=[num_sen,ip_len])\n",
    "        for j in range(0,num_sen):\n",
    "                #ds.shape[0]-(ds.shape[0]%1000)+1)\n",
    "                w1=np.array(ds[j].lower().split())\n",
    "                for i in range(0,ip_len):    \n",
    "                    w=w1[i]\n",
    "                    if i==ip_len-1:\n",
    "                        #w1_sum/=50\n",
    "                        Xtr[j,i]=voc-2#for<end>\n",
    "                        \n",
    "                        break\n",
    "                    if w=='<start>':\n",
    "                        Xtr[j,i]=voc-3\n",
    "                        continue\n",
    "                    if w=='<end>':\n",
    "                        Xtr[j,i]=voc-2#<end>\n",
    "                        i+=1\n",
    "                        while i<ip_len:\n",
    "                            Xtr[j,i]=voc-1#padding\n",
    "                            i+=1\n",
    "                        break\n",
    "                    if w in data:                \n",
    "                        Xtr[j,i]=word_to_ix[w]\n",
    "                    else:\n",
    "                        try:\n",
    "                            temp=word_model.most_similar(positive=[w],negative=[],restrict_vocab=4095)#for<unk>\n",
    "                            idx2+=1\n",
    "                            Xtr[j,i]=word_to_ix[temp[0][0]]\n",
    "                        except KeyError:\n",
    "                            Xtr[j,i]=word_to_ix['it']\n",
    "                            idx+=1\n",
    "\n",
    "\n",
    "                        \n",
    "                           \n",
    "                if j%1000==0:\n",
    "                    print(j)\n",
    "    print(idx)\n",
    "    print(idx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<start>\nyou\nsee\n,\nwe\nthreat\nengaged\n.\n<end>\n<pad>\n<start> you see , we re engaged . <end>\n"
    }
   ],
   "source": [
    "b=Xtr[0]\n",
    "for el in b.flatten():\n",
    "        print(ix_to_word[el])\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 128 samples\nEpoch 1/10\n\nEpoch 00001: accuracy improved from -inf to 0.24453, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 5s - loss: 2.7916 - accuracy: 0.2445\nEpoch 2/10\n\nEpoch 00002: accuracy improved from 0.24453 to 0.25781, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 1s - loss: 2.4268 - accuracy: 0.2578\nEpoch 3/10\n\nEpoch 00003: accuracy improved from 0.25781 to 0.27891, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 2.2598 - accuracy: 0.2789\nEpoch 4/10\n\nEpoch 00004: accuracy improved from 0.27891 to 0.28672, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 2.1602 - accuracy: 0.2867\nEpoch 5/10\n\nEpoch 00005: accuracy improved from 0.28672 to 0.28750, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 2.1028 - accuracy: 0.2875\nEpoch 6/10\n\nEpoch 00006: accuracy improved from 0.28750 to 0.30469, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 2.0181 - accuracy: 0.3047\nEpoch 7/10\n\nEpoch 00007: accuracy improved from 0.30469 to 0.31563, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 1.9734 - accuracy: 0.3156\nEpoch 8/10\n\nEpoch 00008: accuracy improved from 0.31563 to 0.32266, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 1.8905 - accuracy: 0.3227\nEpoch 9/10\n\nEpoch 00009: accuracy improved from 0.32266 to 0.32500, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 1.8524 - accuracy: 0.3250\nEpoch 10/10\n\nEpoch 00010: accuracy improved from 0.32500 to 0.32969, saving model to /home/al/Desktop/nnfl final/weightsfinv6.hdf5\n128/128 - 2s - loss: 1.8253 - accuracy: 0.3297\nTrain on 128 samples\nEpoch 1/10\n\nEpoch 00001: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.6295 - accuracy: 0.2398\nEpoch 2/10\n\nEpoch 00002: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.3974 - accuracy: 0.2531\nEpoch 3/10\n\nEpoch 00003: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.2503 - accuracy: 0.2609\nEpoch 4/10\n\nEpoch 00004: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.1680 - accuracy: 0.2734\nEpoch 5/10\n\nEpoch 00005: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0976 - accuracy: 0.2758\nEpoch 6/10\n\nEpoch 00006: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0873 - accuracy: 0.2719\nEpoch 7/10\n\nEpoch 00007: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.9945 - accuracy: 0.2852\nEpoch 8/10\n\nEpoch 00008: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.9335 - accuracy: 0.3023\nEpoch 9/10\n\nEpoch 00009: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8850 - accuracy: 0.3078\nEpoch 10/10\n\nEpoch 00010: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8491 - accuracy: 0.3125\nTrain on 128 samples\nEpoch 1/10\n\nEpoch 00001: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.6229 - accuracy: 0.2281\nEpoch 2/10\n\nEpoch 00002: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.3665 - accuracy: 0.2438\nEpoch 3/10\n\nEpoch 00003: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.2318 - accuracy: 0.2578\nEpoch 4/10\n\nEpoch 00004: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.1759 - accuracy: 0.2664\nEpoch 5/10\n\nEpoch 00005: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0784 - accuracy: 0.2750\nEpoch 6/10\n\nEpoch 00006: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0073 - accuracy: 0.2812\nEpoch 7/10\n\nEpoch 00007: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.9472 - accuracy: 0.2836\nEpoch 8/10\n\nEpoch 00008: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8979 - accuracy: 0.2891\nEpoch 9/10\n\nEpoch 00009: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8814 - accuracy: 0.2969\nEpoch 10/10\n\nEpoch 00010: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8116 - accuracy: 0.3023\nTrain on 128 samples\nEpoch 1/10\n\nEpoch 00001: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.6534 - accuracy: 0.2234\nEpoch 2/10\n\nEpoch 00002: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.3466 - accuracy: 0.2547\nEpoch 3/10\n\nEpoch 00003: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.2184 - accuracy: 0.2570\nEpoch 4/10\n\nEpoch 00004: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.1223 - accuracy: 0.2648\nEpoch 5/10\n\nEpoch 00005: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0711 - accuracy: 0.2789\nEpoch 6/10\n\nEpoch 00006: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.9914 - accuracy: 0.2750\nEpoch 7/10\n\nEpoch 00007: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.9139 - accuracy: 0.2844\nEpoch 8/10\n\nEpoch 00008: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8571 - accuracy: 0.2945\nEpoch 9/10\n\nEpoch 00009: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8514 - accuracy: 0.2906\nEpoch 10/10\n\nEpoch 00010: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.7915 - accuracy: 0.3023\nTrain on 128 samples\nEpoch 1/10\n\nEpoch 00001: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.3002 - accuracy: 0.2297\nEpoch 2/10\n\nEpoch 00002: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0080 - accuracy: 0.2609\nEpoch 3/10\n\nEpoch 00003: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8794 - accuracy: 0.2664\nEpoch 4/10\n\nEpoch 00004: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.7999 - accuracy: 0.2781\nEpoch 5/10\n\nEpoch 00005: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.7589 - accuracy: 0.2836\nEpoch 6/10\n\nEpoch 00006: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.7086 - accuracy: 0.2914\nEpoch 7/10\n\nEpoch 00007: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.6255 - accuracy: 0.3031\nEpoch 8/10\n\nEpoch 00008: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.5843 - accuracy: 0.3016\nEpoch 9/10\n\nEpoch 00009: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.5821 - accuracy: 0.3086\nEpoch 10/10\n\nEpoch 00010: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.5336 - accuracy: 0.3094\nTrain on 128 samples\nEpoch 1/10\n\nEpoch 00001: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.6200 - accuracy: 0.2211\nEpoch 2/10\n\nEpoch 00002: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.3466 - accuracy: 0.2422\nEpoch 3/10\n\nEpoch 00003: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.1846 - accuracy: 0.2547\nEpoch 4/10\n\nEpoch 00004: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0945 - accuracy: 0.2680\nEpoch 5/10\n\nEpoch 00005: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0605 - accuracy: 0.2586\nEpoch 6/10\n\nEpoch 00006: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 2.0107 - accuracy: 0.2719\nEpoch 7/10\n\nEpoch 00007: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.9044 - accuracy: 0.2836\nEpoch 8/10\n\nEpoch 00008: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.8333 - accuracy: 0.2961\nEpoch 9/10\n\nEpoch 00009: accuracy did not improve from 0.32969\n128/128 - 1s - loss: 1.7871 - accuracy: 0.2977\nEpoch 10/10\n"
    }
   ],
   "source": [
    "#train\n",
    "model.load_weights(filepath)\n",
    "for kl in range(128,num_sen,128):\n",
    "    u=oh(Xtr[kl-128:kl+1],ip_len)\n",
    "    u=tf.convert_to_tensor(u)\n",
    "    \n",
    "    model.fit([u[0:-1],u[0:-1]],u[1:,:,:-1],epochs=10,batch_size=51,use_multiprocessing=True,callbacks=[checkpoint],verbose=2)\n",
    "\n",
    "model.load_weights(filepath)\n",
    "print('\\n\\n\\n\\n\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5032 5030 5023 2290 5023 5023 5023 5033 5033 5033]]\n",
      "<start>\n",
      "i\n",
      ",\n",
      "sad\n",
      ",\n",
      ",\n",
      ",\n",
      "<end>\n",
      "<end>\n",
      "<end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start> exactly two years ago today , she and i buried a time capsule here . <end>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "model.load_weights(filepath)\n",
    "ut=oh(Xtr[0:1],ip_len)\n",
    "a=model.predict([ut,ut])\n",
    "b=np.argmax(a,axis=-1)\n",
    "print(b)\n",
    "for el in b.flatten():\n",
    "        print(ix_to_word[el])\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if any changes to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[]\n",
    "l2=[]\n",
    "for i in range(data2.shape[0]):\n",
    "    if data2[i] in ['are']:\n",
    "        l1+=[i]\n",
    "    else:\n",
    "        l2+=[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=np.loadtxt('/home/al/Desktop/nnflproj/glove.6B.300d.txt',dtype=float,usecols=(range(1,301)),skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix2=dict(zip(data2,range(0,data2.shape[0])))\n",
    "ix_to_word2=dict(zip(range(0,data2.shape[0]),data2))\n",
    "a1=data4[word_to_ix2['is']]-data4[word_to_ix2['do']]+data4[word_to_ix2['did']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vector for had\n",
    "is11=data4[word_to_ix2['has']]+data4[word_to_ix2['did']]-data4[word_to_ix2['do']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.add(entities=data2[1765],weights=data4[1765],replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.save('/home/al/Desktop/nnfl/new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0766063   0.20644    -0.034352   -0.065342    0.27387    -0.664119\n",
      "  0.122819   -0.45537     0.0077661  -1.8518      0.626968   -0.0391655\n",
      " -0.335291    0.52238    -0.0333704   0.385045   -0.172245   -0.007367\n",
      " -0.431167   -0.343461   -0.00482     0.17623    -0.201768   -0.1905209\n",
      " -0.73655     0.134355   -0.44053    -0.41845    -0.38986     0.69412\n",
      "  0.02889     0.12712     0.1865      0.470125   -1.14312    -0.92646\n",
      " -0.051844   -0.02484     0.35317    -0.194072   -0.1537073   0.149239\n",
      " -0.200314    0.35766    -0.0944128   0.469116   -0.71202    -0.355762\n",
      " -0.0886521   0.43724    -0.081358   -0.23484     0.282712    0.515287\n",
      "  0.371574   -0.38545     0.627861    0.48390213  0.565339   -0.62596\n",
      " -0.584631    1.044703    0.125403   -0.73778    -0.0633149  -0.75485\n",
      "  0.005574    0.12999    -0.263811   -0.327531    0.722245   -0.144288\n",
      "  0.548239    0.275       0.21818     0.228203    0.382868    0.1432693\n",
      " -0.6503      0.92362    -0.230969    0.1219582   0.05781     0.376401\n",
      "  0.554199    0.30068     0.12379    -0.429688    0.319806   -0.234184\n",
      " -0.09277    -0.11782    -0.62307     0.44055    -0.047819    0.0450843\n",
      "  0.214788    0.17429     0.139347   -0.44346    -0.114024    0.292238\n",
      "  0.241142    0.16364     0.146996    0.215733    0.26288     0.610343\n",
      " -0.158865    0.156365    0.306572    0.08741     0.293377    0.448147\n",
      " -0.206462   -0.0051235  -0.8205     -0.285781   -0.101875   -0.728939\n",
      " -0.08812    -0.094283    0.071056   -0.25055    -0.04385     0.389415\n",
      " -0.861247    0.063153   -0.3069362   0.878365   -0.32223     0.3147\n",
      "  0.1709     -0.14179     0.023787    0.422319    0.235873    0.154207\n",
      " -0.024439    0.07231    -0.026812    0.176888   -0.012873    0.108792\n",
      " -0.10361    -0.012403    0.203338   -0.173231   -0.4535385  -0.231889\n",
      "  0.73765    -0.101917    0.328353    0.23074    -0.14927     0.168852\n",
      " -0.101727   -0.0717      0.133688    0.10726     0.59709    -0.48274\n",
      " -0.378       0.75003    -0.008671   -0.05471    -0.162309    0.082178\n",
      " -0.027158    0.080445   -0.073101   -0.4019778  -0.42599    -0.268737\n",
      " -0.28244229  0.572123    0.07884    -0.45778     0.17197     0.40753\n",
      " -0.257526    1.192152   -0.107106   -0.17427    -0.18714    -0.42888\n",
      " -0.78536     0.37568     0.018448    0.39763453 -0.53344    -0.1540138\n",
      " -0.20589    -0.514516    0.059009   -0.046339    0.12237     0.49063\n",
      "  0.147392    0.26658     0.9794     -0.67413     0.17152     0.01475\n",
      " -0.214462   -0.210626   -0.47392    -0.05764    -0.218078    0.78014\n",
      "  0.28553     0.438776    0.268106   -0.373214    0.43684    -0.447308\n",
      " -0.88173     0.083403    0.605435   -0.215987    0.85862    -0.014566\n",
      " -0.099919   -0.24567     0.365048    0.36046    -0.235935   -0.56416\n",
      " -0.167172    0.059368   -0.369223   -0.41358    -0.240538    0.26732\n",
      " -0.45155    -0.027926    0.69165     0.031341    0.29677    -0.1612\n",
      "  0.50915    -0.057859   -0.06878    -0.09983    -0.51587    -0.031384\n",
      "  0.05688    -0.20301    -0.14128     0.271131   -0.43388    -0.04807\n",
      "  0.08074    -0.30309     1.02443    -0.408773   -0.2301656  -0.220975\n",
      "  0.211933    0.11695    -0.8547      0.62137     0.052676    0.470427\n",
      " -0.87006     0.589675    0.43218     0.00868     0.10753    -0.251318\n",
      " -0.22448     0.0250289  -0.249165   -0.075806    0.81572    -0.017916\n",
      " -1.5356     -0.303999    0.50856     0.520386   -0.570144    0.153468\n",
      "  0.201181    0.18985    -0.19506    -0.148331   -0.7195      0.3337\n",
      "  0.09696     0.437988   -0.067379    0.16809     0.117111    0.637707\n",
      "  0.008116    1.01926     0.451526   -0.44336    -0.1734      0.0083    ]\n"
     ]
    }
   ],
   "source": [
    "print(is11 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit910a7e525ea849fca55756066482ff27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}